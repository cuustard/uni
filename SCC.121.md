began: 8th October 2024

# SCC.121 Fundamentals of Computer Science

The module aims to help me understand the fundamentals of Computer Science. This includes the role of discrete mathematics and logic, designing algorithms using common data structures, analysing the efficiency of algorithms, and the role of abstract data types.

| Task                 | When                | Worth           |
| -------------------- | ------------------- | --------------- |
| In-lab quiz          | Week 5, 10, 15, 20  | 20% (5% each)   |
| "Summer Project"     | Summer Term         | 10%             |
| Exam                 | Summer Term         | 70%             |

---

### Table of Contents

| Week | My Notes                                                                                                                            | Lecture Slides                                                                                           | Noted |
| :--: | ----------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------- | :---: |
| 1    | [Lecture 1 - Module Introduction & Sets](#lecture-1---sets)                                                                         | [Sets](/SCC.121.slides/b.setsPartOne.pdf)                                                                |  ✅   |
| 1    | [Lecture 2 - Types of Sets](#lecture-2---types-of-sets)                                                                             | [Types of Sets](/SCC.121.slides/c.setsPartTwo.pdf)                                                       |  ✅   |
| 2    | [Lecture 3 - Relations](#lecture-3---relations)                                                                                     | [Relations](/SCC.121.slides/d.relationsPartOne.pdf)                                                      |  🟧   |
| 2    | [Lecture 4 - Relations Part 2](#lecture-4---relations-part-2)                                                                       | [Relations Part 2](/SCC.121.slides/e.relationsPartTwo.pdf)                                               |  ✅   |
| 3    | [Lecture 5 - Functions](#lecture-5---functions)                                                                                     | [Functions](/SCC.121.slides/f.functions.pdf)                                                             | ❌    |
| 3    | [Lecture 6 - Functions Part 2](#lecture-6---functions-part-2)                                                                       | [Functions Part 2](/SCC.121.slides/g.functionsPartTwo.pdf)                                               | ❌    |
| 4    | [Lecture 7 - Propositional Logic](#lecture-7---propositional-logic)                                                                 | [Propositional Logic](/SCC.121.slides/h.propositionalLogic.pdf)                                          |  ✅   |
| 4    | [Lecture 8 - Propositional Logic Part 2](#lecture-8---propositional-logic-part-2)                                                   | [Propositional Logic Part 2](/SCC.121.slides/i.propositionalLogicPartTwo.pdf)                            |  ✅   |
|  5   | [Lecture 9 - Predicate Logic](#lecture-9---predicate-logic)                                                                         | [Predicate Logic](/SCC.121.slides/j.predicateLogic.pdf)                                                  |  ❌   |
|  5   | [Lecture 10 - Predicate Logic Part 2](#lecture-10---predicate-logic-part-2)                                                         | [Predicate Logic Part 2](/SCC.121.slides/k.predicateLogicPartTwo.pdf)                                    |  ❌   |
|  6   | [Lecture 11 - Introduction to Data Structures & Abstract Data Types](#lecture-11---introduction-to-data-types--abstract-data-types) | [Introduction to Data Structures & Abstract Data Types](/SCC.121.slides/l.introDataStructuresAndADT.pdf) |  ✅   |
|  6   | [Lecture 12 - Memory, Pointers, & Records](#lecture-12---memory-pointers--records)                                                  | [Memory, Pointers, & Records](/SCC.121.slides/m.memoryPointersRecords.pdf)                               |  ✅   |
|  7   | [Lecture 13 - Two Dimenstional Arrays](#lecture-13---two-dimensional-arrays)                                                        | [Two Dimensional Arrays](/SCC.121.slides/n.TwoDArrays.pdf)                                               |  ✅   |
|  7   | [Lecture 14 - The Kitchen Sink](#lecture-14---the-kitchen-sink)                                                                     | [The Kitchen Sink](/SCC.121.slides/o.theKitchenSink.pdf)                                                 |  n/a  |
|  8   | [Lecture 15 - The Stack](#lecture-15---the-stack)                                                                                   | [The Stack](/SCC.121.slides/p.stack.pdf)                                                                 |  ✅   |
|  8   | [Lecture 16 - Linked Lists](#lecture-16---linked-lists)                                                                             | [Linked Lists](/SCC.121.slides/q.linkedLists.pdf)                                                        |  ✅   |
|  9   | [Lecture 17 - Searching](#lecture-17---searching)                                                                                   | [Searching](/SCC.121.slides/r.searching.pdf)                                                             |  ✅   |
|  9   | [Lecture 18 - Recursion](#lecture-18---recursion)                                                                                   | [Recursion](/SCC.121.slides/s.recursion.pdf)                                                             |  ✅   |
|  10  | [Lecture 19 - Indexed Retrieval](#lecture-19---indexed-retrieval)                                                                   | [Indexed Retrieval](/SCC.121.slides/t.indexedRetrieval.pdf)                                              |  ✅   |
|  11  | [Lecture 20 - Hashing](#lecture-20---hashing)                                                                                       | [Hashing](/SCC.121.slides/u.hashing.pdf)                                                                 |  ✅   |
|  11  | [Lecture 21 - Introduction To Algorithms](#lecture-21---introduction-to-algorithms)                                                 | [Intro To Algorithms](/SCC.121.slides/v.introToAlgorithms.pdf)                                           |  ✅   |
|  12  | [Lecture 22 - Introduction To Operations Counting](#lecture-22---introduction-to-operations-counting)                               | [Intro To Operations Counting](/SCC.121.slides/w.operationCounting.pdf)                                  |  ✅   |
|  12  | [Lecture 23 - Operating Counting Part 2](#lecture-23---operating-counting-part-2)                                                   | [Operation Counting Part 2](/SCC.121.slides/x.operationCountingPrt2.pdf)                                 |  ❌   |
|  13  | [Lecture 24 - Linear Search: Time Complexity](#lecture-24---linear-search-time-complexity)                                          | [Linear Search: Time Complexity](/SCC.121.slides/y.linearSearchTimeComplexity.pdf)                       |  🟧   |
|  13  | [Lecture 25 - Sentinel & Binary Search Algorithms](#lecture-25---sentinel--binary-search-algorithms)                                | [Sentinel & Binary Search Algorithms](/SCC.121.slides/z.sentinelAndBinarySearch.pdf)                     |  🟧   |
|  14  | [Lecture 26 - Big O Notation](#lecture-26---big-o-notation)                                                                         | [Big O Notation](/SCC.121.slides/za.bigO.pdf)                                                            |  ❌   |
|  14  | [Lecture 27 - Big O Notation Part 2](#lecture-27---big-o-notation-part-2)                                                           | [Big O Notation Part 2](/SCC.121.slides/zb.bigO2.pdf)                                                    |  ❌   |
|  15  | [Lecture 28 - Big Ω & Θ Notation](#lecture-28---big-ω--θ-notation)                                                                  | [Big Ω & Θ Notation](/SCC.121.slides/zc.BigOmegaAndTheta.pdf)                                            |  ✅   |
|  15  | [Lecture 29 - Time Complexity of Recursive Algorithms](#lecture-29---time-complexity-of-recursive-algorithms)                       | [Recursive Time Complexity](/SCC.121.slides/zd.timeComplexityRecursion.pdf)                              |  ✅   |
|  16  | [Lecture 30 - Sorting, Trees, & Graphs](#lecture-30---sorting-trees--graphs)                                                        | [Sorting, Trees, & Graphs](/SCC.121.slides/ze.SortingGraphsTrees.pdf)                                    |  🟧   |
|  16  | [Lecture 31 - Sorting, Trees, & Graphs Part 2](#lecture-31---sorting-trees--graphs-part-2)                                          | [Sorting, Trees, & Graphs Part 2](/SCC.121.slides/zf.SortingGraphsTrees2.pdf)                            |  🟧   |
|  17  | [Lecture 32 - Sorting, Tress, & Graphs Part 3](#lecture-32---sorting-trees--graphs-part-3)                                          | [Sorting, Trees, & Graphs Part 3](/SCC.121.slides/zg.SortingGraphsTrees3.pdf)                            |  🟧   |
|  17  | [Lecture 33 - Sorting, Tress, & Graphs Part 4](#lecture-33---sorting-trees--graphs-part-4)                                          | [Sorting, Trees, & Graphs Part 4](/SCC.121.slides/zh.SortingGraphsTrees4.pdf)                            |  ❌   |
|  18  | [Lecture 33 - Sorting, Tress, & Graphs Part 5](#lecture-34---sorting-trees--graphs-part-5)                                          | [Sorting, Trees, & Graphs Part 5](/SCC.121.slides/zi.SortingGraphsTrees5.pdf)                            |  ❌   |
|  18  | [Lecture 34 - Sorting, Trees, & Graphs Part 6](#lecture-35---sorting-trees--graphs-part-6)                                          | [Sorting, Trees, & Graphs Part 5](/SCC.121.slides/zj.SortingGraphsTrees6.pdf)                            |  ❌   |
|  19  | [Lecture 35 - Greedy Algorithm](#lecture-36---greedy-algorithm)                                                                     | [Greedy Algorithm](/SCC.121.slides/zk.greedyAlgorithm.pdf)                                               |  ❌   |
|  19  | [Lecture 36 - Dynamic Programming](#lecture-37---dynamic-programming)                                                               | [Dynamic Programming](/SCC.121.slides/zl.dynamicProgramming.pdf)                                         |  ❌   |

> **_NOTE:_** The AI Podcasts used for each lecture are produced from that lecture's slides. The structure of my notes for each lecture may differ to the order of topics discussed in the podcast.

## Lecture 1 - Sets

A Set is a collection of unique and unordered objects/elements/members. E.g. `A = {4, 3, 2, 1, 5}`

### Membership

Elements in a set have a membership to that set.

For example, the element 4 is in set A. Write this as `4 ∈ A`. This means 4 'belongs to' or 'is an element of' set A.

However, the element 6 is not in set A. Write this as `6 ∉ A`. This means 6 'does not belong to' or 'is not an element of' set A.

### Defining Sets

Finite and small sets are easy to write out. For example, set A: `A = {4, 3, 2, 1, 5}` is finite with just 5 elements.

Infinite/large sets cannot be enumerated. Instead, we provide a property that all the set's elements satisfy.

For example, set B: `B = {1, 2, 3, 4, 5, 6...}` and so on until infinity. To write this, we can give this set a property of x. We would say that, every element of x such that x is an integer and greater than 0. So `B = {x | P(x)}`. This means that B is the set of elements x such that x has the property P.

### Set Operations

- Union: `∪`
- Intersection: `∩`
- Difference `-`
- Cartesian Product `x`

For the following examples: `A = {1, 2, 3, 4}`, `B = {4, 5, 6}`.

Unionisation forms a new set from two sets, made of all the elements from both. Any duplicates of an element are removed. For example, `A ∪ B = {1, 2, 3, 4, 5, 6}`. Here we unionised two sets into one and removed duplicate element, 4, at the same time.

Intersection forms a new set from two sets, made of the elements that are common between A and B. For example, `A ∩ B = {4}`. This is because both sets have element 4, and nothing else, in common.

Difference forms a new set from two sets, made of all the elements from the first set that are not in the second set. For example. `A - B = {1, 2, 3}`.

To understand Cartesian Product, first I need to understand Ordered Pairs:

- An Ordered Pair is a pair of elements, with an order (usually ascending) associated with them
- An Ordered Pair is written as `<x, y>`. Where x and y are elements
- Two Ordered Pairs `<a, b>` and `<c, d>` are equal if `a = c` and `b = d`. This means that `<1, 2>` is not equal to `<2, 1>`

The Cartesian Product of two sets creates a set of all possible ordered pairs between the sets. For example:

```
A x B =
{
 <1, 4>, <1, 5>, <1, 6>,
 <2, 4>, <2, 5> <2, 6>,
 <3, 4>, <3, 5>, <3, 6>,
 <4, 4>, <4, 5>, <4, 6>
}
```

Summary of set operations:

| Symbol  | Symbol name        | Meaning                                                                        |
| ------- | ------------------ | ------------------------------------------------------------------------------ |
| A ∪ B   | Union              | Elements that belong to set A **or** set B                                     |
| A ∩ B   | Intersection       | Elements that belong to set A **and** set B                                    |
| A - B   | Difference         | Elements that belong to set A but not set B                                    |
| A x B   | Cartesian Product  | All ordered pairs with the first element from set A and the second from set B  |

## Lecture 2 - Types of Sets

Types of sets:

- Empty
- Disjoint
- Equal
- Sets of sets
- Subsets & Proper Subsets
- Supersets & Proper Supersets
- Universal
- Complement

### Empty Sets

An empty/null/void set contains no elements. Written as `{}` or `∅`. For example `Z = {}` can also be written as `Z = ∅`.

### Disjoint Sets

Sets are disjoint if they have no elements in common (if their intersection is empty). For example, `C = {1, 2, 3}` is disjoint from `D = {4, 5, 6}`. This is because they have no common elements.

### Equal Sets

Sets are equal if they have the same elements. For example `E = {1, 2, 3}` is equal to `F = {2, 3, 1}`. Written as `C = D`. If sets are unequal use the `≠` symbol. For example, `D ≠ E`.

### Sets of Sets

Sets can contain atomic elements like letters, numbers, or a pair of elements. They can also contain other sets. For example take `A = {1, {2, 3}}`. Set `A` contains the atomic element `1`, and the set `{2, 3}`.

### Cardinality of Sets

The Cardinality of a set is the number of a set's elements. They are written as `|A|`. For example, `A = {1, 2, 3, 4, 5}` so `|A| = 5`. For example, `B = {1, {1, 2}, ∅, 4}` so `|B| = 4`.

### Subsets & Supersets

Set A is a subset of set B if every element of A is also an element of B. This is written as `A ⊆ B`. This would mean that if `x ∈ A`, then `x ∈ B` In some cases A could also be equal to B.

For example, if `A = {1, 2, 3}` and `B = {1, 2, 3, 4, 5}` then:

- `1 ∈ A` and `1 ∈ B`
- `2 ∈ A` and `2 ∈ B`
- `3 ∈ A` and `3 ∈ B`

All of A's elements are in B so `A ⊆ B`.

This makes B a superset of A. Written `B ⊇ A`.

#### Proper Subsets & Proper Supersets

A is also a proper subset of B because B has some elements not in A. So `A ⊂ B`. This makes B a superset of A. Written as `B ⊃ A`. This also means that for a Proper Subset, `A ≠ B`.

### Universal Sets

A Universal set is a non-empty set that contains all the possible elements relevant to the solution of a given problem. Example: `U = {red, orange, yellow, green, blue, indigo, violet}`.

### Complement Sets

A complement set is the difference between the universal set and a given set. Denoted by `comp(A) = U - A`. For example if `A = {red, yellow, blue}` then `comp(A) = {orange, green, indigo, violet}`.

Summary of Type of sets:

| Symbol   | Symbol name      | Meaning                                                                        |
| -------- | ---------------- | ------------------------------------------------------------------------------ |
| ∅        | Empty Set        | Set with no elements                                                           |
|          | Disjoint Sets    | Sets whose intersection is the empty set                                       |
| A = B    | Equal Sets       | Sets with the same elements                                                    |
| A ≠ B    | Unequal Sets     | Sets which do not have the same elements                                       |
| A        | Set Cardinality  | Number of elements in a set                                                    |
| A ⊆ B    | Subset           | Elements of set A are also in set B                                            |
| A ⊂ B    | Proper Subset    | A is a subset and there is at least one element in set B that is not in set A  |
| B ⊇ A    | Superset         | Elements of set A are also in set B                                            |
| B ⊃ A    | Proper Superset  | B is superset and there is at least one element in set B that is not in set A  |
| U        | Universal Set    | Set of all of the possible elements relevant to a specific problem             |
| comp(A)  | Complement Set   | The difference between the universe and a given set A                          |

## Lecture 3 - Relations

![Associations](images/associations.png "Diagram showing example of assocations between students and academic courses")

Pairs of objects are related in specific ways. We can represent a relationship by describing what course each student takes. Starting from the student and ending with the course. We specify the relationship this way because it is a student that takes a course; a course does not take a student.

Relations are asymmetric. The direction matters because the order of elements in the order pair matters.

The cartesian product will give us all possible ordered pairs (12 in this example). But there are only 5 relations. In real life, which students take which course could be all the possible pairs, but often it will be a subset of the cartesian product.

A binary relation is an ordered pair where two elements are brought together.

Binary Relation R from set A to set B, or over `A x B`. A set of ordered pairs `<a, b>`, `a ∈ A` and `b ∈ B`. An ordered pair `<a, b> ∈ R`. This means element a is related to element b through the relation R.

The relationship between R and A x B is as follows:

- A x B is the set of all ordered pairs (the cartesian product)
- R is a subset of `A x B`. So `R ⊆ A x B`. It is not a proper subset as R could be the cartesian product.

In the above diagram, Relation T (Takes):

- John Takes course1
    - `<John, course1> ∈ T`
- Jim Takes course1
    - `<Jim, course1> ∈ T`
- Helen Takes course2 and course3
    - `<Helen, course2> ∈ T`
    - `<Helen, course3> ∈ T`
- Mary Takes course3
    - `<Mary, course3> ∈ T`

Another example:

- `A = {0, 1, 2, 3}` and `B = {0, 1, 2, 3, 4}`
- List the ordered pairs in the relation R from A to B that satisfy `<a, b> ∈ R`, if `b - a = 1`.
- `b - a = 1` is equivilant to `b = a + 1`. So `<a, b> = <a, a + 1>`:
- So the relation R looks like: `R = {<0, 1>, <1, 2>, <2, 3>, <3, 4>}`

## Lecture 4 - Relations Part 2

### Operations on Relations

| Symbol  | Symbol name               | Meaning                                                                                          |
| ------- | ------------------------- | ------------------------------------------------------------------------------------------------ |
| R1 ∪ R2 | union of relations        | set of all ordered pairs `<a, b>` that are in R1, or R2, or both                                 |
| R1 ∩ R2 | intersection of relations | set of all ordered pairs `<a, b>` that are common to both R1 and R2                              |
| R1 - R2 | difference of relations   | set of all ordered pairs `<a, b>` that are in R1 but not in R2                                   |
| R1 ⊆ R2 | subrelation               | R1 is subrelation of R2 if every ordered tuple that is an element of R1 is also an element of R2 |

### Properties of Relations

### Properties of Relations

| Property      | Definition                                                                                                      | Explanation                                                                                                             | Example                                                                                                                                                         |
| ------------- | --------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Symmetry      | `R ⊆ A x A` is symmetric if for any `a` and `b` in `A`, if `<a, b> ∈ R` then `<b, a> ∈ R`.                      | There is symmetry if for any `<a, b>`, there is also a `<b, a>`. Otherwise, there is no symmetry.                       | "is a sibling of" is a symmetrical comparison, whereas "is a brother of" is not. `Jake is a sibling of Sophie` is symmetrical to `Sophie is a sibling of Jake`. |
| Transitivity  | `R ⊆ A x A` is transitive for any `a`, `b`, and `c` in `A`, if `<a, b> ∈ R` and `<b, c> ∈ R` then `<a, c> ∈ R`. | There is transitivity if, for any `<a, b>` and `<b, c>` in `R`, then `<a, c>` should also be in `R`.                    | The `=` relation is transitive. If `a = b` and `b = c`, then `a = c`.                                                                                           |
| Reflexivity   | `R ⊆ A x A` is reflexive if `<a, a> ∈ R` for every element `a` of `A`.                                          | There is reflexivity if, for any `a` in `A`, then `<a, a>` should be in `R`. Every element of `A` is related to itself. | Let `B = {5, 6, 7}`. For `R` to be reflexive it must contain `<5, 5>, <6, 6>, <7, 7>`.                                                                          |
| Irreflexivity | `R ⊆ A x A` is irreflexive if `<a, a> ∉ R` for every element `a` of `A`.                                        | No element of `A` is in relation to itself.                                                                             | "is the parent of" is an irreflexive relation. No one can be their own parent.                                                                                  |
| Equivalence   | `R ⊆ A x A` is an equivalence relation if `R` is reflexive, symmetric, and transitive.                          |                                                                                                                         |                                                                                                                                                                 |

## Lecture 5 - Functions

A function is a machine which given an input produce a unique output. Deterministic linkage ebtween two sets of values: inputs an outputs.

It is a special type of binary relation. It associates each element of a set with a unique element of anotehr set.

Abstract relation between sets. Each input value corresponds to a unique output value. Output vlaue depends in some way on the input value.

### Formal Defintiions of Fuinctions

Function from set A to B: for every `a ∈ A`, there is a unique `b ∈ B` such that `<a, b> ∈ f`. Notation: `f : A → B`.

A function from set A to set B is a relation from A to B that satisifes:

- for each element a in A, there is an element b in B such that `<a, b>` is in the relation, and that element is unique: if `<a, b>` and `<a, c>` are in the relation, then `b = c`.

### Domain, Codomain, & Range

Let the function f from A to B be: `f : A → B`.

- A is the **domain** of function f - all input elements
- B is the **codomain** of function f - all possible output elements

The **range** is the set of values that are produced by a function. Therefore the range is a subset of the codomain: `range ⊆ codomain`.

### Image & Preimage

## Lecture 6 - Functions Part 2

## Lecture 7 - Propositional Logic

Logic is the study of reasoning. It is a rational way of drawing conclusions.

A proposition is a claim about how things are. It can either be true OR false. Not both. For example:

True propositions:

- Grass is green
- Snow is white
- 2 + 2 = 4

False propositions:

- Grass is red
- Snow is pink
- 2 + 2 = 5

The following are examples of non-propositions

- Is the water warm?
- Go for it!
- Where are we?
- Put the phone down!
- Ouch!

An Atomic proposition is one that is true or false value does not depend on that of any other proposition.

A compound proposition is one which is constructed from atomic propositions by combining them with fundamental connectives.

### Truth Tables

These tabulate the value of a compound proposition for all possible values of its atomic propositions and their combination. For example, the truth table for 2 atomic propositions:

| P   | Q   | Compound  |
| --- | --- | --------- |
| F   | F   |           |
| F   | T   |           |
| T   | F   |           |
| T   | T   |           |

### Fundamental Connectives

| Operation      | Symbols  |
| -------------- | -------- |
| AND            | ⋀        |
| OR             | ⋁        |
| XOR            | ⊕, ⊻     |
| NOT            | ~, ¬     |
| Conditional    | →        |
| Biconditional  | ⇔        |

#### AND

![AND Connective](images/connectiveAND.png)

#### OR

![OR Connective](images/connectiveOR.png)

#### XOR

![XOR Connective](images/connectiveXOR.png)

#### NOT

![NOT Connective](images/connectiveNOT.png)

#### Conditional / Implication

IF antecedent THEN consequent. For example: IF the train is late, THEN we will miss our flight.

This combines two propositions into a third proposition called the conditional or implication. The conditional/implication is false when the antecedent is True and the consequent is False.

![Conditional / Implication](images/conditionalImplicational.png)

#### Biconditional

The biconditional is only True when both propositions have the same truth value.

![Biconditional](images/biconditional.png)

### Logical Properties

Tautologies are propositions which are always True, regardless of the truth values of their atomic propositions. For example, Q = "I passed the exam". ~Q = "I did not pass the exam!.

![Tautologies](images/tautologies.png)

Contradictions are propositions which are always false, regardless of the truth values of their atomic propositions. For example, Q = "I passed the exam". ~Q = "I did not pass the exam!.

![Contradictions](images/contradictions.png)

Contingencies are propositions that are neither tautologies nor contradictions. For example, P = "I passed the exam". ~P = "I did not pass the exam".

Contingencies have both True's and Falses in their truth tables.

![Contingencies](images/contingencies.png)

Equivalence is when two propositions are logically equivalent if they have the exact same truth value under all circumstances. Written `P ≡ Q`.

## Lecture 8 - Propositional Logic Part 2

### Logical Reasoning

- An Argument is a sequence of propositions that end with a conclusion.
    - The argument is VALID if, given that the premises are true, then the conclusion is true.
- A Premise is the basis on which we establish the conclusion.
- A Conclusion is a claim that we try to establish as true.

Written in the form:

![Logcal Reasoning Writen Form](images/logcialReasoning.png)

### Propositional Logic

Building blocks of propositional logic:

- Atomic & Compound Propositions
- Fundamental Connectives

A rule is a function that takes propositions as premises and returns others as conclusions.

- **Inference Rules**: templates for building valid arguments
- **Replacement Rules**: replaces parts of propositions with logically equivalent expressions

### Inference Rules

Inference rules give methods to evaluate the validity of arguments. They highlight logical reasoning behind a valid argument are justify stems from premises to conclusion. An alternative to this is truth tables, but these are inconvenient with more propositions and are less intuitive.

The rules:

| Rule                                      | Description                                                                                                                                                                                                         | Example Image                                                                                                                 |
| ----------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- |
| Modus Ponens                              | one premise is a conditional statement, the other premise affirms the antecedent and the conclusion affirms the consequent                                                                                          | ![Modus Ponens Example](images/modusPonens.png "Diagram showing example of Modus Ponens Rule")                                |
| Modus Tollens                             | one premise is a conditional statement, the other premise denies the consequent, and the conclusion denies the antecedent                                                                                           | ![Modus Tollens Example](images/modusTollens.png "Diagram showing example of Modus Tollens Rule")                             |
| Addition (disjunction introduction)       | the premise is a proposition, and the conclusion is a disjunction formed by that proposition and any other proposition                                                                                              | ![Addition Example](images/addition.png "Diagram showing example of Addition Rule")                                           |
| Simplification (conjunction elimination)  | the premise is a conjunction, and the conclusion is either of the propositions forming the conjunction                                                                                                              | ![Simplification Example](images/simplification.png "Diagram showing example of Simplification Rule")                         |
| Hypothetical Syllogism                    | the premises are two conditionals such as P → Q and Q → R so that one’s antecedent matches the consequent of the other, and the conclusion is another conditional which results from the chain of reasoning: P → R  | ![Hypothetical Syllogism Example](images/hypotheticalSyllogism.png "Diagram showing example of Hypothetical Syllogism Rule")  |
| Disjunctive Syllogism                     | one premise is a disjunction, the other premise denies one of the propositions in the disjunction, and the conclusion affirms the other proposition in the disjunction                                              | ![Disjunctive Syllogism Example](images/disjunctiveSyllogism.png "Diagram showing example of Disjunctive Syllogism Rule")     |
| Absorption                                | the premise is a conditional: P → Q, and the conclusion is also a conditional whose consequent is a conjunction of the consequent and antecedent: P → ( P ∧ Q )                                                     | ![Absorption Example](images/absorption.png "Diagram showing example of Absorption Rule")                                     |

### Replacement Rules

| Rule/Law             | Description                                                                                                                                                                                                   | Example                                                                   |
| -------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------- |
| Commutative law      | The order of laws does not affect the result of the conjunction or disjunction                                                                                                                                | `P ∨ Q` = `Q ∨ P` / `P ∧ Q` = `Q ∧ P`                                     |
| Associative law      | The grouping of propositions does not affect the result of the conjunction or disjunction                                                                                                                     | `(P ∨ Q) ∨ R` = `P ∨ (Q ∨ R)` / `(P ∧ Q) ∧ R` = `P ∧ (Q ∧ R)`             |
| Distributive law     | 'Multiply' out the brackets                                                                                                                                                                                   | `P ∧ (Q ∨ R)` = `(P ∧ Q) ∨ (P ∧ R)` / `P ∨ (Q ∧ R)` = `(P ∨ Q) ∧ (P ∨ R)` |
| De Morgan’s laws     |                                                                                                                                                                                                               | `~P ∧ ~Q` = `~(P ∨ Q)` / `~P ∨ ~Q` = `~(P ∧ Q)`                           |
| Absorption law       | The disjunction of any proposition P with (P ∧ Q) has the same truth value as P                                                                                                                               | `P ∨ (P ∧ Q)` = `P` / `P ∧ (P ∨ Q)` = `P`                                 |
| Identity law         | The conjunction of any proposition P with an arbitrary tautology T has the same truth value as P. <br> The disjunction of any proposition P with an arbitrary contradiction F has the same truth value as P   | `P ∧ T` = `P` <br> `P ∨ F` = `P`                                          |
| Idempotence law      | The property of a conjunction or disjunction to be applied multiple times on a proposition without changing the proposition                                                                                   | `P ∧ P` = `P` / `P ∨ P` = `P`                                             |
| Negation law         | The disjunction of any proposition P and its negation is a tautology                                                                                                                                          | `P ∨ ~P` = `True` / `P ∧ ~P` = `False`                                    |
| Double negation law  |                                                                                                                                                                                                               | `~(~P)` = `P`                                                             |
| Implication law      | Implication can be expressed by disjunction and negation                                                                                                                                                      | `P → Q` = `~P ∨ Q`                                                        |
| Contraposition law   |                                                                                                                                                                                                               | `P → Q` = `~P → ~Q`                                                       |
| Equivalence law      | A biconditional is equivalent to the conjunction of two conditionals                                                                                                                                          | `P <--> Q` = `(P → Q) ∧ (Q → P)`                                          |

## Lecture 9 - Predicate Logic

here...

## Lecture 10 - Predicate Logic Part 2

here...

## Lecture 11 - Introduction to Data Types & Abstract Data Types

A structure being an Abstract Data Type (ADT) tells us what operations can be performed, but now how they will be implemented. This means the ADT does not specify how the data will be organisd in memory, or the algorithms used for implementing the operations.

It is abstract because it is an implementation independent view.

Abstraction is the process of provoding only the necessary information and hiding the internal details is known as abstraction.

### The Queue

A queue is an ordered collection of items which are added at the tail, and removed from the head. It is a First In First Out (FIFO) ordering property.

| Operations    | def                                                 | Parameters     | Returns                                |
| ------------- | --------------------------------------------------- | -------------- | -------------------------------------- |
| Queue()       | Creates new empty queue                             | None           | Empty queue                            |
| enqueue(item) | Adds new item to the tail of the queue              | An item to add | Nothing                                |
| dequeue()     | If not empty, removes item in the head of the queue | None           | Item removed from the queue            |
| isEmpty()     | Tests if the queue is emppty                        | None           | Boolean True if empty. False otherwise |
| size()        | Coutns number of items in the queue                 | None           | Number of items in the queue           |

![Queue Evolution](images/queueEvolution.png "Diagram showing queue procedures/functions")

### Data Structures vs Abstract Data Types

A data structure is the physical representation of the structure of the datya being stored in memory.

An abstract data type is both the data structure and the procedures/functions which manipulate that data structure.

### Encapsulation

Encapsulation wraps attributes and methods in a class, hiding the details from the user.

The only way the user can interact with a variable of that ADT is through an interface (a set of procedures that operate on the data structure).

Encapsulation is strong if the details of how the data structure is actually stored can be completely hidden from the user. The user will have a conceptual (abstract) understanding of what the ADT does, but not how it does it.

C does not have encapsulation. So instead we have to declare a new data structure to model an ADT then provide it with procedures/functions that operate on the data structure.

### Arrays

```C
// Declaring an array in C
int counts[26] = {a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z};
```

![Array Table](images/alphabetTable.png)

```C
// to increment the count for 'l' we can write
counts[11] = counts[11] + 1;

//print the value of the count for 'h'
printf("%d\n", counts[7]);
```

- upb (upper bound) stores the index of the last element.
- lwb (lower bound) stores the index of the first element. (always 0)

## Lecture 12 - Memory, Pointers, & Records

Computer memory is **byte addressable**. This means every byte of memory has 1 address. Thought of as an array. This can be thought of as an array where the address is the index of the array.

A **word**'s length can differ between machines. We'll assume a word is 32 bits (4 bytes) long. An integer is usually 1 word long.

**Variable**'s have 3 components:

- A symbolic **name**
- A **value** it contains
- An **address** where it is in memory

If a variable is on the left of the statement, it is assignment, on the right it is fetch.

### Pointers

C variables have local scope to the function they appear in. This makes it harder for multiple functions to work with - and acctually affect the value in a variable - the same variable. For example there may be a function to create a student record, and another that modifies that record. Functions pass variables by value which means that they copy the value into the function so that the actual value stored at the variables memory address is not affected.

Pointers fix this by allowing us to pass the memory address of a variavble to the function as well as the value.

```C

// define character variable, and pointer variable
char val;
char *addr;

// initialise by assigning values
val = 3;
addr = &val;
printf("val = %d, addr = %x\n", val, addr);

*addr = *addr + 2; // deference means go to address and fetch contents. so go to content stored at address and add 2
printf("val = %d, addr = %x\n", val, addr);

// Outputs:

// val = 3, addr = 4063e8
// val = 5 addr = 4063e8
```

### Levels of Indirection

```C
val = val + 2;

// fetch - get content from the location where RHS val resides
// evaluate - add 2 to it
// store - the value in the location where val resides
```

### Notation

```C
//indicate that a variable is a pointer with the * "type modifier"

int* x;
int * x;
int *x; // can be read as "x is a pointer to an int

// all above are equivalent
```

```C
// unary operator * is the dereference operator (indirection operator)

*ptr; // fetches the value stored at ptr
%val; // returns address of val
```

### Records

Records are compound and heterogeneous (multile data types in it). A records components are called fields and each field is identified with a name (rather than an index). It holds many properties of a single entity.

```C
// define record with MAXSIZE 20

typedef struct student { //typedef: type definition. // struct: a structure // student: name of the tstruct
  char name[MAXSIZE];
  int age;
  char gender;
  int entryYear;
  char subject[MAXSIZE];
  char maritalStatus;
} Student; // Student: name of the type
```

Function that creates space for a new Student record and returns a pointer to that space.

```C
Student* newStudent()
//allocates space for a new Student record
//returns pointer to allocated space
{
Student* pt = malloc(sizeof(Student));
return pt;
};
```

#### Arrow -> Operator

The following creates a new Student record and allocates values to the fields

```C
Student* stu = newStudent();
strcpy(stu->name, "James T. Kirk");
stu->age = 19; // stu -> age is the same as (*stu).age = 19
stu->gender = 'M';
stu->entryYear = 2252;
strcpy(stu->subject, "Space Command");
```

#### Dot . Operator

Dot is used to access the fields rather than the arrow to allocate values.

```C
Student stu2;
strcpy(stu2.name, "Nyota Uhura");
stu2.age = 18;
stu2.gender = 'F';
stu2.entryYear = 2257;
strcpy(stu2.subject, "Communications");
stu2.maritalStatus = 's';
```

#### Arrays of Records

Can have an array of students.

```C
Student* arrayOfStudents[2];
arrayOfStudents[0] = stu;
arrayOfStudents[1] = &stu2;
```

#### "set" Functions

Function to only allow certain values for martial status

```C
bool setMaritalStatus(Student* s, char x)
{
  bool ok = false;
  switch (x)
  {
    case 'm': case 'w':
    case 's': case 'd': ok = true; break;
  };
  if (ok) s->maritalStatus = x;
  return ok;
}
```

## Lecture 13 - Two Dimensional Arrays

Elements in a 2D array are accessed by two indexes - one for a row, the other for a column.

```C
rating[0][2] // [0] specifies the row - [2] specifies the column
```

Declaring in C:

```C
int arrayName[3][4]; // creates empty arraw with 3 rows and 4 columns.

// another example with char instead of integers

char characterArray[2][3] // empty array with 2 rows and 3 columns
// each element is type char
```

Generating each pair of inicies:

```
for (int row = 0; row < 2; row++) {
  for (int col = 0; col < 3; col++) {
    print row, col;
  }
}
```

Initialising the 2D array:

```C
void initCounts() {
  for (int r = 0; r < NUMBER_OF_LANGS; r++) {
    for (int c = 0; c < NUMBER_OF_LANGS; c++) {
    counts[r][c] = 0;
    }
  }
  counts[18][18] = 569;
  counts[18][22] = 32;
  counts[22][18] = 24;
  counts[22][22] = 743;
}
```

Examining elements of 2D array:

```C
void printCounts() {
  for (int r = 0; r < NUMBER_OF_LANGS; r++) {
  for (int c = 0; c < NUMBER_OF_LANGS; c++) {
    if (counts[r][c] != 0) {
      printf("%d, %d = %d\n", r, c, counts[r][c]);
      }
    }
  }
}
```

Summing the total number of samples in our 2D array:

```C
int getTotal() {
  int total = 0;
  for (int r = 0; r < NUMBER_OF_LANGS; r++) {
    for (int c = 0; c < NUMBER_OF_LANGS; c++) {
      total = total + counts[r][c];
    }
  }
  return total;
}
```

Summing the correct values:

```C
int getTotalCorrect() {
  int correct = 0;
  for (int l = 0; l < NUMBER_OF_LANGS; l++) {
    correct = correct + counts[l][l];
  }
  return correct;
}
```

Finding the accuracy of the classifier:

```C
void printAccuracy() {
  int total = getTotal();
  int totalCorrect = getTotalCorrect();
  printf("\nAccuracy: %d/%d: %.2f%%\n", totalCorrect, total, ((double)totalCorrect / (double) total) * 100.0);
}
```

| Pros                                                          | Cons                              |
| :------------------------------------------------------------ | :-------------------------------- |
| Use single name to represent many data items of the same type | Fixed size                        |
| Random access so very fast                                    | Insertion and deletion are costly |

## Lecture 14 - The Kitchen Sink

## Lecture 15 - The Stack

A stack is an Abstract Data Type (ADT) where the collection of items is ordered by when they were added. It follows the Last In First Out (LIFO) principle.

It has functions `push` and `pop`. `Push` places an item on the top of the stack. `Pop` removes the item on the top of the stack and returns it.

### What Can Go Wrong?

- Trying to pop an empty stack causes an underflow error. The `pop` method needs to check if the stack is empty before trying to pop.
- In bounded stacks, trying to push an item onto a full stack causes an overflow error. The `push` method needs to check if the stack is full before trying to push.

Each element has a value and a pointer to the previous element.

### Unbounded Stack

Initialize empty stack:

```
Stack() {
  top = nil;
}
```

An element:

```
Element {
  int data;
  Element prev;
}
```

Check if the stack is empty:

```
bool Empty(S) {
  if (S.top == nil) {
    return true;
  }
  return false;
}
```

Push element to an unbounded stack:

```
Push(S, x) {
  el = new Element;
  el.data = x;
  el.prev = nil;
  if (Empty(S)) {
    S.top = el;
  } else {
    el.prev = S.top;
    S.top = el;
  }
}
```

Pop element from an unbounded stack:

```
int Pop(S) {
  if (Empty(S)) {
    "Underflow";
  } else {
    tmp = S.top;
    S.top = S.top.prev;
    return tmp.data;
  }
}
```

### Bounded Stack

Initialize empty stack:

```
Stack() {
  top = -1;
}
```

Check if stack is empty:

```
bool Empty(S) {
  if (S.top == -1) {
    return true;
  }
  return false;
}
```

Check if stack is full:

```
bool Full(S) {
  if (S.top == MAX_SIZE - 1) {
    return true;
  }
  return false;
}
```

Push element to a bounded stack:

```
Push(S, x) {
  if (Full(S)) {
    "Overflow";
  } else {
    S.top++;
    S.A[S.top] = x;
  }
}
```

Pop element from a bounded stack:

```
int Pop(S) {
  if (Empty(S)) {
    "Underflow";
  } else {
    data = S.A[S.top];
    S.top--;
    return data;
  }
}
```

## Lecture 16 - Linked Lists

A list is an Abstract Data Type (ADT) that stores a set of items in a linear order. In our case, there are no duplicates.

### Doubly Linked list

```C
Element {
  Item data
  Element next, prev // pointers to Element
}
```

```C
Add(L, Element e) // add element e to L

remove(L, e) // remove e from L

Element search(L, Item k) // returns pointer to Element containing k if it is present in L, else nil

int size(L) // Return count of items in L
```

Initially, the head of a List is nil, indicating that the list is empty

```C
List() {
  Element head = nil
}
```

Each element has 3 'fields', previous element, the data, next element.

Implementation:

```C
search(L, k) {
  p = L.head
  while(p != nil && p.data != k) {
    p = p.next
  }
  return p
}

add(L, e) {
  e.next = L.head // adding to the front
  if(L.head != nil) {
    L.head.prev = e
  }
  L.head = e
  e.prev = nil
}


remove(L, e) {
  if (e.prev != nil) { // not the first element
    e.prev.next = e.next
  } else {
    L.head = e.next
  }
  if (e.next != nil) { // not the last element
    e.next.prev = e.prev
  }
}
```

### Singly Linked List

No prev pointer. And unlike doubly linked list, it can only be traversted in forward direction. so removing an item alwats requires traversal.

- Insert and remove at particular position in list.
- Remove by data
- Iterators: getFirst() and getNext()
- E.G. tail pointer may be used to point to end of list and make a reverse traversal even more convenienet.
- Sentinelts (Special markers) may be ued to simplify code

## Lecture 17 - Searching

### Data Storage and Retrieval

- Put data item in store
- Retrieve Specific Item

If storage is quick, retrievel is slow. If Storage is careful, retrievel will be fast.

| Storage                                                      | Retrieval                                     |
| ------------------------------------------------------------ | --------------------------------------------- |
| **Fast**: append latest data to the end of the store         | **Slow** linear search to find required item  |
| **Slow**: append latest data to sorted position in the store | **Fast**: binary search to find required item |

### Theoretical Time Complexity

This is the relationshpi between the size of the input and the algorithm running time.

- Let N be the size of the input
- Characterise the runtime of the algorithm as a function of N

The relationship tells us, for example, if the amount of data is doubled does the algorithm take...

- 2x as long to run - then acceptable
- more than 2x as long to run - then very bad
- less than 2x as long to run - then very good

#### Big-O Classes

![BigO Notation Cheat Sheet](images/BigONotationCheatSheet.jpg)

![Time Complexity Graph](images/timecomplexitygraph.jpg)

### Linear Searching

For an unsorted linear array A containing N integers:

1. Find index of first occurance of integer x
2. If x is not present in array then return -1

```
ls(a[], x) {
  for i=0 to a.length {
    if(a[i] = x) {
      return i
    }
  return -1
  }
}
```

If we let N be the size of the input array, then the linear seach algorithm has complexity O(N). This is a linear compelxity as runtime grows in direct proportion to the input.

### Binary Search

Can only be used on sorted array. It has better theoretica time complexity than linear search as even if the array is sorted for a linear search it still has complexity O(N).

To find X in array A:

1. look at element A[M]. M is midpoint of array A.
2. If X == A[M] then it is found
3. If size(A) == 1 AND X != A[M] then X is not in A
4. If X < A[M] X is in the left half of A. Jump back to step 1 for LHS.
5. If X > A[M] X is in the right hald of A. Jump back to step 1 for RHS.

```C
// search for X in sorted array A
int low = 0;
int high = N - 1;
int mid; // next element to try

while(low <= high) {
  mid = (low + high)/2;
  if (A[mid] == X) {
    return mid;
  }
  if (X < A[mid]) {
    high = mid - 1; // narrow down to RHS
  } else {
    low = mid + 1; // narrow down to LHS
  }
}
return -1; // not found in array
```

Binary Search Complexity: O(log<sub>2</sub>N)

## Lecture 18 - Recursion

A recursive fuction is one which calls itself in itself. We can use iteration or recursion to run the same piece of logic a given number of times.

```C
void counter(int n) {
  for (int i = 0; i < n; i++) {
    print("loop")
  }
}
```

```C
void counter(int n) {
  if (n != 0) {
    print("loop");
    counter(n - 1);
  }
}
```

### Order of Execution

The order in which we execute logic in a recursive function depends on whether its before or after the self call.

logic-before-recursion:

```C
void counter(int n) {
  if (n != 0) {
    print("loop " + (n - 1));
    counter(n - 1)
  }
}

// Outputs:
// loop 3
// loop 2
// loop 1
// loop 0
```

logic-after-recursion:

```C
void counter(int n) {
  if (n != 1) {
    counter(n - 1);
    print("loop " + (n - 1));
  }
}

// Outputs:
// loop 0
// loop 1
// loop 2
// loop 3
```

### Advantages

Iteration:

- good: memory usage is controlled explicitly by the programmer, so a stack overflow is less likey
- good: can execute quicker as there is no overhead from stack frame creation/destruction
- bad: naturally recursive function can be harder to understand in an iterative form

Recursion:

- good: naturally recursive functions are much more concise when expressed this way
- good: languages which support tail recursion can eliminate some of the extra cost to performance, and stack overflows

### Examples

Iterative:

```C
int total(int array[]) {
  int result = 0;

  for (int i = 0; i < array.length; i++) {
    result += array[i];
  }
  return result
}
```

Recursively:

```C
int total(int array[], int size) {
  if (size == 1) {
    return array[size - 1]
  }
  return array[size -1] + total(array, size -1)
}

// Say array = [5, 10, 15, 20]; size = 4
// 20 + total([5, 10, 15, 20], 3)
// = 20 + (15 +  total([5, 10, 15, 20], 2))
// = 20 + (15 +  (10 + total([5, 10, 15, 20], 1))
// = 20 + (15 +  (10 + (5)))
// = 20 + (15 +  (15))
// = 20 + (30)
// = 50
```

Factorial function e.g.:

```C
int factorial(int n) {
  int total = 1;
  for (int i = 1; i <= n; i++) {
    total = total * i
  }
  return total;
}
```

Recursively:

```C
int factorial(int n) {
  if (n == 0) {
    return 1;
  }
  return n * factorial(n - 1);
}
```

## Lecture 19 - Indexed Retrieval

### Keys

Indexed retrieval requires an identifier to store and retireve each object or record. An attribute of each object can be used as a key for storage and retrieval. The **key** is a unique name/identifier for each object. Keys allows a set of objects/records to be stored in a sorted order, so binary search can be used for retrieval.

An array of sorted keys can fall into natural 'sections'. For example, there would be some A record, some B record, some C record, etc. A new array can be constructed to index the first element for each section. index[0] holds the index of the first A record. index[1] holds the index for the first B record. index[2] holds the index of the first C record. etc. If there are no keys with a specific letter then index[n] = -1.

![Index Mapping Example](images/indexMappingExample.png)

Here we can see that the index[n] for first key entry is the index of the first appearence of n in the data store array. the index array points to particular sections in the data store.

Lets say we wanted to find the name Andy. We would look at the index[0], linear search until we find Andy at index[3]. Lets say we want to find the name Chris. Instead of searching through the entire data store array. We know only to look at the C entrys, We go to index[6] then linear search until we find it.

```
take the first L from name;
convert L to numeric value V;
P = index[V];

if(P < 0>);
  then no records start with this letter so fail;

while(the first letter of the key of STORE[P]==L) {
  if(key == name) {
    return STORE[P] // found it
  }
  P++; // move to next record
}

// if loop exhausts then the name was not found
```

Indexed retrieval with linear seach has worst case efficiency of O(n). this is because in the worst case, all the names start with the same letter, therefore all records are in the same section, so we have to linear search through the entire array.

But in practice, because some names begining with some letters are more common than others, the gain will be probably smaller.

If some sections are large, we can do a binary search of the section starting with STORE[P]. This has worst case of O(log<sub>2</sub>n). This is because if everything is in one section, we have to binary search the entire array.

But in practice, it depends on the size of sections but could be up to 26x faster than binary search without indexing.

### Collisions

There is a challange with indexing. Collisions can occur.

### Collision-free Storage

Imagine we have a set of words each with a different starting letter. This would be an array of 26 elements with the index of each word determined by its first letter. It would take the first letter of the word to test. Check it in the appropriate array element.

This is O(1) algorithm.

```
Given name, look for record in store with matching key:
  take first letter L from name;
  convert L to numeric value V;
  if(STORE[V].key == name) {
    return STORE[V]; // record found
  }
  else... // no matching key
```

### Adding Entry

#### Solution 1: indexed retrieval

What if we need to add an entry for Christophe? We would have to make room for the entry in the store. This means that all the entries from position 0 onwards have to be shifted into the next slot. Then we can add christophe.

![yada](images/addingEntryIndexedRetireval.png)

But once we have Christophe in there, the index is wrong from element 3 onwards. So we have to recompute the Index by adding 1 to every posotion value starting at element 3.

![yada](images/addingEntry2indexedRetrieval.png)

#### Solution 2: using chains

For storage based on the first letter of each key we need to use a linear array STORE of 26 elements representing the alphabet with each element pointing to the appropriate chain; i.e. STORE[0] points to a chain of objects whose keys starts with 'A'.

```
Given name, look for record in STORE with matching key:
  take first letter L from namel;
  convert L to numeric value V;
  obtain chain pointer P from STORE[V];
  while(P != null) {
    if (P.key == name) {
      return P; // found the record
    }
    P = P.next; // move to next record
  }
```

This has worst case efficiency of O(n). As, if every object is on the same chain then linear search through the entire thing.

But practically, it is much faster if the objects are distributed across many chains so the same improvement as with using arrays.

Advantage as chains are dynamic so extra objects can be inserted easier. No need to recompute the index.

## Lecture 20 - Hashing

<audio controls>
  <source src="SCC.121.slides/u.hashing.mp3" type="audio/mpeg">
  Your browser does not support the audio element.
</audio>

A hashing function converts a key (usually a string) into an integer. The integer is then used for indexting a sotrage array. The same function is used for storing and retrieving data.

Index integer is not just restricted from 0 to 25, which allows dada to be spread accross a much longer array.

Collisions can be dealt with chaining.

A hash table is a data structure designed for fast data retrieval. Elements in a hash table are stored in 'buckets' (e.g., an index of an array). Each element in the table has a unique part called the key. A hash function takes the key of an element and generates a hash code. The hash code determines which bucket the element belongs to, allowing for direct access to that element in the table.

A good hashing function is one that minimises collisions by having a wide range of index values and unform hashing (each key is equally likely to map to an array location). To get a wide range of index values we can use multiplication as well as addition.

Hash retrieval has two stages:

1. Use hashing function to compute index
2. Search chain in array element to see if there is a match

A good hashing function and a large array could make stage 2 close to O(1), but stage 1 should be fast too. A real-world hashing function should avoid slow multiplication and division operations, and instead use fast logicial shifts and OR operations.

### Key-value paris

So far we've just been storing the keys. But we can also store an associated value as a key-value pair.

This is when each induvidual element of data (the value) is associated with a unique identifier. We use a label (the key) to identify a piece of information (the value). E.g. the name (key) has associated phone number (value).

- **Key**: Unique input to hash function (can be anything e.g. integer, but often a string) that is translated by hash function
- **Hash of the key**: Determines the bucket where we store the value
- **Value**: what we put in the bucket associated with the key (in previous examples we just put the key itself in the bucket)

## Lecture 21 - Introduction To Algorithms

<audio controls>
  <source src="SCC.121.slides/v.introToAlgorithms.mp3" type="audio/mpeg">
  Your browser does not support the audio element.
</audio>

### What An Algorithm Is

> "An algorithm is a set of computational steps that transform the input into output... A tool for solving well-specified computational problem... The algorithm describes a specific computational procedure for achieving that input/output relationship." - Cormen, Thmas H., et al. Introduction to algorithms. MIT press, 2009

An algorithm is an effective method for solving a class of problems. The problem input is finite and represents an instance of a problem.

Algorithms can be represented in computer code, pseudocode, flow charts, plain text, punch cards, etc.

### Examples Of Algorithms

There are many different approaches to solving the same problem. The inputs and outputs may be the same but the steps taken to compute the ouput can differ.

Key types of algorithms:

- **Sorting Algorithms**: Sorts data in a particular format. E.g. Quicksort, Mergesort, etc.
- **Searching Algorithms**: Finds a requested value/record. E.g. Linear search, Binary search, etc.
- **Graph Algorithms**: Finds solutions to problems such as the shortest path between 2 points. E.g. breadth-first, Dijkstra's algorithm, etc.

Some algorithms are better than others. We can the performance of algorithms by measuring their time/space complexity.

### Why We Study Algorithms

Algorithms need resources during execution. They require **space** (memory), and **time** (number of steps relative to input size). Performance is important but so are correctness, robustness, and simplicity.

Even if computers were infintely fast and memory was free, we should still study algorithms as they should still be correct, robust, an simple.

Ideally we want to minimise space and time complexity of an algorithm. But in reality, it's more likely that we'll only be able to greately one of these. XOR situation.

### Space Complexity

Space complexity is the amount of memory required by an algorithm to run to completion. Space can be split into two parts:

1. **Fixed Part**: The size needed to store data/variables, that is independent of the size of the prolem.
2. **Variable Part**: Space needed by variables, whose size is dependent on the size of the problem.

```C
// in this example, lets assume the size of each int variable is 4 bytes. there are 3 ints so 4*3=12. so 12bytes of fixed space is needed. There is no variable part.
#include <stdio.h>

int main() {
  int a = 5, b = 5, c;
  c = a + b;
  printf("%d", c)
}

```

```C
// in this example, the fixed part is 12 bytes. the arr consists of n ints. so the space is 4n cause 4 bytes. so space complexity of 12 + 4n
#include <stdio.h>

int main() {
  int n, i, sum = 0;
  scanf("%d", &n);
  int arr[n];
  for (i = 0, i <n; i++) {
    scanf("%d", &arr[i]);
    sum = sum + arr[i];
  }
  printf("%d", sum);
}

```

### Space Vs Time

An ideal algorithm requires less space and takes less time to compute. In practice there is a trade-off time vs space complexity. Generally we care more about time complexity as memory is inexpensive even though it is not free. Time is important as we want it to be as fast as possible and the speed depends on the size and organisation of the input.

## Lecture 22 - Introduction To Operations Counting

<audio controls>
  <source src="SCC.121.slides/w.operationCounting.mp3" type="audio/mpeg">
  Your browser does not support the audio element.
</audio>

### Running Time

The running time depends on the size and organisation of the input. We seek upper bounds (worst case) on the running time.

### Cost Of Operations

To determine how long an algorithm takes to run, we can count the operations it executes. To determine what 1 operation is, we ignore differences between different programming languages and compilers and only analyse the ideas of the algorithm itself.

For example these would count as 1 operation each:

```c
x = a + b;
x = theArray[i];
x = x + theArray[i];
n < theArray[i];
i++;
```

### Operation Counting & T(N)

```C
int maxel = arr[0]; // 1 Operation
for (int i = 0; i < n; i++) { // 2 Operations
  if (arr[i] >= maxel) { // 1 Operation in the for
    maxel = arr[i]; // 1 operation in the for
  }
}

// 3 operations outside the for = 3. 2 operatins in the for = 2n.
// 3 + 2n
```

![image](images/operationCounting.png)

T(n) cannot be defined easily becaose the number of operations isnt dependent on just the size of input, but also on the organisation of the input. For the above algorithm `arr = [1, 2, 3, 4]` will take longer than `arr = [4, 3, 2, 1]`.

Another example:

```C
int sum = 0;

for (int i = 0; i < k; i++) {
  sum += a[i];
}
```

Before the loop starts, we do `int sum = 0;` which is 1 operation, then `int i =0;` which is a second, then `i < k;` which is a third. Then if `i < k;` is true, the loop operates and we do `i++;`, `sum += a[i]`, and the comparison `i < k;` again. That sums 3 before the loop starts, then 3 during the loop. thats 3 + 3n.

## Lecture 23 - Operating Counting Part 2

<audio controls>
  <source src="SCC.121.slides/x.operationCountingPrt2.mp3" type="audio/mpeg">
  Your browser does not support the audio element.
</audio>

## Lecture 24 - Linear Search: Time Complexity

<audio controls>
  <source src="SCC.121.slides/y.linearSearchTimeComplexity.mp3" type="audio/mpeg">
  Your browser does not support the audio element.
</audio>

General for loop:

```
for (i = a; i <= b; i++>) {
  // block
}
```

$$
\displaystyle\sum_{i=a}^b 1=b - a + 1
$$

### Linear Search Worst, Best, & Average Case

**Worst Case** is the usually the one we look at for analysis. In this case, we calculate the upper bound on the runnign time of an algorithm. We must know the case that causes a max number of operations to be executed.

**Average Case** is looked at less often. Useful if worst case rarely occurs. We take all possible inputs and calculate the computing time for all of the inputs. Sum all the calculated values and divide the sum by the total number of inputs.

**Best Case** is looked at less often aswell. We calculate the lower bound on the running time of an algorithm. We must know the case that causes a minimum number of operations to be executed.

Linear Search Algorithm:

```C
int isInArray(int theArray[], int N, int iSeaerch) {
  for (int i = 0; i < N; i++) {
    if (theArray[i] == iSearch) {
      return 1;
    }
  }
  return 0;
}
```

![image](images/linearSearchOperationCounting.png)

o1, o5, and o6 are done once in the entire function. o2, o3, and o4, are executed with each loop.

| case (best to worst)           | o2  | o3  | o4  |
| :----------------------------- | :-: | :-: | :-: |
| 'iSearch' is the first element |  1  |  1  |  0  |
| 'iSearch' is the last element  |  N  |  N  | N-1 |
| 'iSearch is not in 'theArray'  | N+1 |  N  |  N  |

Best Case `T(N) = 1 + 1 + 0 + 1 + 1 + 0 = 4`

![image](images/linearSearchBestCase.png)

To calculate the average case, we must do `1/N * (1 + 2 + 3 + 4 + 5 + ... + N) = `
1/n

$$
\frac{1}{N} \displaystyle\sum_{i=1}^N i
$$

![image](images/linearSearchAverageCase.png)

Worst Case `T(N) = 1 + (N + 1) + N + N + 0 + 1 = 3N + 3`

![image](images/linearSearchWorstCase.png)

## Lecture 25 - Sentinel & Binary Search Algorithms

<audio controls>
  <source src="SCC.121.slides/z.sentinelAndBinarySearch.mp3" type="audio/mpeg">
  Your browser does not support the audio element.
</audio>

### Linear Search s Sentinel Search

Linear:

```C
int isInArray(int theArray[], int N, int iSearch)
{
  for (int i = 0; i < N; i++) {
    if (theArray[i] == iSearch) {
      return 1;
    }
  }
  return 0;
}
```

When a linear search is done on an array of size N, then in the worst case a total of N+1 comparisons are made for the index of the element to be compared so that the index is not out of bounds of the array.

A sentinel search is a type of Linear Search where the number of comparisons is reduced as compared to the linear search.

MISSING NOTES HERE

### Binary Search

Binary search only works on sorted array. It locates a target value in a sorted array by successively eliminating half of the array from consideration.

Iterative implementation of Binary Search:

```C
boolean isInBinary(int[] theArray, int N, int iSearch)
{
  int lo = 0; // o1
  int hi = N - 1; //o1
  int mid = 0; // 03
  while (hi >= lo) { // o4
    mid = (lo + hi)/2; //round to higher integer // o5
    if (theArray[mid] == iSearch) // o6
      return true; // o7
    else if (theArray[mid] < iSearch) // o8
      lo = mid + 1; // o9
    else
      hi = mid - 1; //o10
  }
  return false; // o11
}
```

Worst case scenario is if the target value is not in the array. And if o6 is always False and we assume o8 is always false.

Best case complexity of constant. as if the element is first thing found. so not dependant on size of array.

- Sentinel search and Linear search algorithms are both linear in worst case scenario, but Sentinel search requires executing fewer operations.
- Binary search algorithm is more efficient comparing to Linear and Sentinel Search algorithms (Binary logarithmic in worst case), but the input array should be sorted.
- Actual values of constants generally unimportant (except in specific circumstances)
- What we really care about is behaviour as the size of our input increases (asymptotic behaviour)

### Growth of Functions

Assume two algorithms A and B that solve the same class of problems. The time complexity of A is T(n) = 500n, for B it is T(n) = 1.1<sup>n</sup> for an input with n elements.

Algorithm B grows exponentially which means that it should not be used for large inputs while A is still feasable. The growth of time complexity with increasing input size n is a suitable measure for the comparison of algorithms.

![image](images/growthOfFunctionsTable.png)

![image](images/growthOfFunctionsPlot.png)

Growth Rate of Functions (best to worst):

1. 1 → Constant growth
2. log n → Logarithmic growth
3. nc → where 0<c<1
4. n → Linear growth
5. n log n
6. n<sup>2</sup> → Quadratic growth
7. n<sup>2</sup> log n
8. n<sup>3</sup> → Cubic growth
9. n<sup>c</sup> → Polynomial growth (c is a constant number)
10. 2<sup>n</sup> → Exponential growth
11. 3<sup>n</sup> → Exponential growth
12. c<sup>n</sup> → Exponential growth (c is a constant number)
13. n! → Factorial growt

## Lecture 26 - Big O Notation

<audio controls>
  <source src="SCC.121.slides/za.bigO.mp3" type="audio/mpeg">
  Your browser does not support the audio element.
</audio>

## Lecture 27 - Big O Notation Part 2

<audio controls>
  <source src="SCC.121.slides/zb.bigO2.mp3" type="audio/mpeg">
  Your browser does not support the audio element.
</audio>

## Lecture 28 - Big Ω & Θ Notation

<audio controls>
  <source src="SCC.121.slides/zc.BigOmegaAndTheta.mp3" type="audio/mpeg">
  Your browser does not support the audio element.
</audio>

### Asymptotic Growth

#### The Big Ω Notation

- Let T(n) and f(n) be posotive functions from the integers or the real numbers to the real numbers.
- T(n) is Ω(f(n)) if, even as n becomes arbitrarily large, T(n)'s growth is bounded from below by f(n), meaning it grows no slower than f(n).
- T(n) ∊ Ω(f(n)) if there are posotive constants M and n<sub>0</sub> such that: `T(n) >= m * f(n)` for all n >= n<sub>0</sub>

#### The Big Θ Notation

- Let T(n) and f(n) be positive functions from the integers or the real numbers to the real numbers.
- T(n) is the Θ(f(n)) if, even as n becomes arbitrarily large, T(n)'s growth is bounded from above and below by f(n), meaning it grows no faster than f(n).
- T(n) ∊ Θ(f(n)) if there are positive constants M<sub>1</sub>, M<sub>2</sub> and n<sub>0</sub> such that: M<sub>1</sub> _ f(n) <= T(n) <= M<sub>2</sub> _ f(n) for all n >= n<sub>0</sub>

![images](images/bigNotationsCompare.png)

### Linear Seaarch Complexity Notations

```C
int isInArray(int theArray[], int N, int iSearch) {
  for(int i = 0; i < N; i++) {
    if (theArray[i] == iSearch) {
      return 1;
    }
  }
  return 0;
}
```

- **Best Case**: T(N) = 4
  - T(N) = Constant --> O(1) Ω(1) Θ(1)
- **Average Case**: T(N) = (3P/2 + 3 - 3P)N + (5P/2 + 3 -3P)
  - T(N) = C<sub>1</sub> \* N + C<sub>2</sub> --> O(N) Ω(N) Θ(N)
- **Worst Case**: T(N) = 3N + 3
  - T(N) = C<sub>1</sub> \* N + C<sub>2</sub> --> O(N) Ω(N) Θ(N)
  - C<sub>1</sub> and C<sub>2</sub> are constant

![image](images/linearSearchComplexitiesNotation.png)

## Lecture 29 - Time Complexity of Recursive Algorithms

<audio controls>
  <source src="SCC.121.slides/zd.timeComplexityRecursion.mp3" type="audio/mpeg">
  Your browser does not support the audio element.
</audio>

A recursive algorithm is an algorithm which calls itself with smaller input values. It obtains the result from the current input by aplpying simple operations to the returned value for teh smaller input. If a problem can be solved utilising solutions to smaller versions of the same problem, and the smaller versions reduce to easily solvable cases, then one can use a recursive to solve that problem.

### Examples

### Evaluating the Time Complexity of Recursive Algorithms

#### Back Substitution

Finding the time complexity of the sum function can then be reduced to solving the recurrence relation. We want to find T(n) in terms of n.

- `T(1) = 1`
- `T(n) = T(n - 1) + 1 when n > 1`

From this we can know that:

- `T(n - 1) = T(n - 2) + 1`
- `T(n - 2) = T(n - 3) + 1`
- `T(n - 3) = T(n - 4) + 1`

Then using the substitution:

- `T(n) = T(n - 1) + 1 = T(n - 2) + 1 + 1 = T(n - 2) + 2`
- `T(n) = T(n - 2) + 2 = T(n - 3) + 1 + 2 = T(n - 3) + 3`

Continuing the substitution, and remembering from equation 2, we wrote:

- `T(n - 1) = T(n - 2) + 1`
- `T(n - 2) = T(n - 3) + 1`
- `T(n - 3) = T(n - 4) + 1`

Next, using substitution:

- `T(n) = T(n-1) + 1 = T(n-2) + 1 + 1 = T(n-2) + 2`
- `T(n) = T(n-2) + 2 = T(n-3) + 1 + 2 = T(n-3) + 3`
- `T(n) = T(n-3) + 3 = T(n-4) + 1 + 3 = T(n-4) + 4`

Notice that if we continue the substitution, in general we can write:

- `T(n) = T(n - k) + k`

We can use equation 1 of our recurrence relation:

- `T(1) = 1`
- `T(n) = T(n - 1) + 1 when n > 1`

From this, we know the value of `T(1) = 1`. We want `(n - k) = 1`, so we can use `T(1) = 1. (n - k) = 1` can be rearranged to give `k = n - 1`. Subbing `k = (n - 1)` gives `T(n) = T(n - k) + k = T(1) + n - 1 = 1 + n - 1 = n`. So `T(n) = n`. Which means the algorithm is `θ(n)`.

#### Recursion Tree

Trees are special cases of graphs having no loops; having only one path between any two vertices (nodes). The depth of a tree is the depth of the deepest node. The branching factor is the number of children at each node.

![image](images/trees.png)

A recursion tree is useful for visualising what happens when a recurrence is iterated. It diagrams the tree of recursive calls and the amount of work done at each call.

Given the following recurrence relation:

- T(1) = 1
- T(n) = 2T(n/2) + 1

We can use a recursion tree to find the time complexity.

![image](images/recursionTree.png)

![image](images/recursionTree2.png)

![image](images/recursionTree3.png)

![image](images/recursionTree4.png)

![image](images/recursionTree5.png)

![image](images/recursionTree6.png)

![image](images/recursionTree7.png)

![image](images/recursionTree8.png)

#### Master Theorem

Let T(n) be a monotonically increasing function that satisfies:

- T(n) = aT(n/b) + f(n)
- T(1) = c

where a >= 1, b >= 2, c > 0 and f(n) is Θ(n<sup>d</sup>) where d >= 0 then:

![image](images/masterTheorem.png)

You cannot use the Master Theorem if:

- T(n) is not monotone, e.g. T(n) = sin(x)
- f(n) is not polynomial, e.g. T(n) = 2T(n/2) + 2<sup>n</sup>
- b cannot be expressed as a constant

## Lecture 30 - Sorting, Trees, & Graphs

<audio controls>
  <source src="SCC.121.slides/ze.SortingGraphsTrees.mp3" type="audio/mpeg">
 Your browser does not support the audio element.
</audio>

- Input: list/array of data in an arbitary order
- Output: list/array in a sorted order

The data may consist of duplixates and we have no info on the distribution of the data (e.g. its highest and lowest values or the median).

Sorting data is fundamental to computational problems. Ranking data, or to find top X items, searching and other operations are faster on sorted datasets. If many searches are done on the same dataset, sorting it first makes for an efficient preprocessing step.

### Selection Sort

<iframe width="560" height="315" src="https://www.youtube.com/embed/g-PGLbMth_g?si=8iviam6V0WNLhG-1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

E.G. of a shit selection sort:

- Input Array: `[5, 4, 2, 5, 6, 1, 8, 7]`
- Sorted Array: `[ , , , , , , , ]`

Find the minimum value from the input array, remove it, and add it to the sorted array.

- Input Array: `[5, 4, 2, 5, 6, , 8, 7]`
- Sorted Array: `[1, , , , , , , ]`

Repeat:

- Input Array: `[5, 4, , 5, 6, , 8, 7]`
- Sorted Array: `[1, 2, , , , , , ]`

Repeat:

- Input Array: `[5, , , 5, 6, , 8, 7]`
- Sorted Array: `[1, 2, 4, , , , , ]`

Repeat:

- Input Array: `[, , , 5, 6, , 8, 7]`
- Sorted Array: `[1, 2, 4, 5, , , , ]`

Repeat:

- Input Array: `[, , , , 6, , 8, 7]`
- Sorted Array: `[1, 2, 4, 5, 5, , , ]`

Repeat:

- Input Array: `[, , , , , , 8, 7]`
- Sorted Array: `[1, 2, 4, 5, 5, 6, , ]`

Repeat:

- Input Array: `[, , , , , , 8, ]`
- Sorted Array: `[1, 2, 4, 5, 5, 6, 7, ]`

Repeat:

- Input Array: `[, , , , , , , ]`
- Sorted Array: `[1, 2, 4, 5, 5, 6, 7, 8]`

In-place algorithms use little additional space like O(1). How do we sort in-place:

<iframe width="560" height="315" src="https://www.youtube.com/embed/g-PGLbMth_g?si=8iviam6V0WNLhG-1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

Code:

```C
class SelectionSort {
    static void selectionSort(int[] array){
        for (int i = 0; i < array.length; i++) {
            int iMin = i;      
            for (int j = i + 1; j < array.length; j++) {  //find index of minimum
                if (array[j] < array[iMin]) iMin = j;
            }
            if (iMin != i) {                //swap array[i] and array[iMin]
                int tmp= array[i];
                array[i] = array[iMin];
                array[iMin] = tmp;
            }
        }
    } }
```

## Lecture 31 - Sorting, Trees, & Graphs Part 2

<audio controls>
  <source src="SCC.121.slides/zf.SortingGraphsTrees2.mp3" type="audio/mpeg">
 Your browser does not support the audio element.
</audio>

### Merge Sort

The merge sort is a divide an conquer Paradigm. The idea is to decompose a problem into a simpler sub-problems (of the same type). Then, solve recursively on the sub-problems. Finally, combine the solutions for the sub-problems to get a solution of the original problem.

1. Split: the input array into two equal halves.
2. Solve: Recursively solve (sort) the two subarrays independently
3. Combine: Combine the two sorted subarrays by merging to get an overall sorted array

![image](images/mergeSortSimpified.png)

See powerpoint for longy-slided detailed explanation for the steps. I aint typing alla that.

tbc...

## Lecture 32 - Sorting, Trees, & Graphs Part 3

<audio controls>
  <source src="SCC.121.slides/zg.SortingGraphsTrees3.mp3" type="audio/mpeg">
 Your browser does not support the audio element.
</audio>

A Tree is a non-linear abstract data type (ADT) which stores elements hierarchically.

### Applications for Trees

Computer document formats like XML or HTML describe a tree of relationships between elements.

![image](images/HTMLtree.png)

A compiler takes source code and converts it into an abstract syntax tree to make it easy to evaluate the program in the correct order.

![image](images/compilerTree.png)

Physic simulations, image processing, and game design use tree ADTs for mesh generation (computer graphics), collision detection, etc.

### Definitions

A tree is nodes connected by edges with not cycles or loops.

Hierarchical data structure from root node, to its children, and so on, until a leaf (node without children) is reached.

Descendants of a node are any nodes below it (children, grandchildren, etc). So this means all nodes are descendants of the root node. Vice versa for Ancestors.

Tree split into levels (each level like a different generation).

![image](images/treeDistances.png)

Height of a tree is the number og generations (exlucding root node) (basically number of levels). Diameter/width of a tree is the number of nodes on longest path between any two leaves.

A k-ary tree is one that imposes a max number of children to each node. Binary trees are especially popular with a max of 2 children per node.

## Lecture 33 - Sorting, Trees, & Graphs Part 4

<audio controls>
  <source src="SCC.121.slides/zh.SortingGraphsTrees4.mp3" type="audio/mpeg">
 Your browser does not support the audio element.
</audio>

## Lecture 34 - Sorting, Trees, & Graphs Part 5

<audio controls>
  <source src="SCC.121.slides/zi.SortingGraphsTrees5.mp3" type="audio/mpeg">
 Your browser does not support the audio element.
</audio>

## Lecture 35 - Sorting, Trees, & Graphs Part 6

<audio controls>
  <source src="SCC.121.slides/zj.SortingGraphsTrees6.mp3" type="audio/mpeg">
 Your browser does not support the audio element.
</audio>

## Lecture 36 - Greedy Algorithm

<audio controls>
  <source src="SCC.121.slides/zk.greedyAlgorithm.mp3" type="audio/mpeg">
 Your browser does not support the audio element.
</audio>

## Lecture 37 - Dynamic Programming
